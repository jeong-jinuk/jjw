{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모듈 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from urllib import parse # 주소 파싱 위해서\n",
    "from urllib.parse import quote_plus\n",
    "import requests\n",
    "import urllib.request\n",
    "import Augmentor\n",
    "import glob, os\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "import keras.applications.xception as xception\n",
    "from keras.preprocessing import image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getter_url = 'https://thepopularlist.com/most-famous-paintings-in-the-world/'\n",
    "source = urlopen(url).read()\n",
    "source = BeautifulSoup(source, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터를 가져올 주소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫번째 공백으로 분리하여 추가\n",
    "keys = []\n",
    "values = []\n",
    "fm_name = {}\n",
    "a = source.find_all('h2')\n",
    "for i in range(len(a)-2):\n",
    "    keys.append(a[i+1].text[:a[i+1].text.find(\" \")+1])\n",
    "    values.append(a[i+1].text[a[i+1].text.find(\" \")+1:])\n",
    "    fm_name[keys[i]] =  values[i]\n",
    "\n",
    "fm_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터의 이름들을 확인하기 위한 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = source.find_all('img',class_='lazy')\n",
    "str(c[61]).split('\\\"')[7]\n",
    "src_url = []\n",
    "n = 0\n",
    "for i in range(len(c) - 2):\n",
    "    n += 1\n",
    "    print(n, str(c[i+2]).split('\\\"')[7])\n",
    "    src_url.append(str(c[i+2]).split('\\\"')[7])\n",
    "src_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "for i in range(len(src_url)):\n",
    "    n+=1\n",
    "    print(n)\n",
    "    m = parse.quote(src_url[i][8:])\n",
    "    try:\n",
    "        if not os.path.isdir('./fmp'):\n",
    "            os.mkdir('./fmp')\n",
    "    except OSError as e :\n",
    "        if e.errno != errno.EEXIST:\n",
    "            print('Failed to create Directory!!')\n",
    "    with urlopen('https://' + m) as f:\n",
    "        with open('./fmp/' + values[i]+'.jpg','wb') as h: # w - write b - binary\n",
    "            img = f.read()\n",
    "            h.write(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 확인한 데이터의 url을 추출하고 위의  url을 통해 이미지를 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmp_name = {}\n",
    "n = 0\n",
    "for x in glob.glob(\"./fmp/*.*g\"): \n",
    "    n += 1\n",
    "    fn = os.path.basename(x)\n",
    "    dn = os.path.basename(x).rsplit('.jpg')[0]\n",
    "    dn = dn.split('/')[-1]\n",
    "    fmp_name[n] = dn\n",
    "    img_src = Image.open(x)\n",
    "    # resize\n",
    "    res_img = img_src.resize((299,299)) #max img size (299, 299)\n",
    "    try:\n",
    "        if not (os.path.isdir('./resize/')):\n",
    "            os.mkdir('./resize/')\n",
    "        if not (os.path.isdir('./resize/'+dn)):\n",
    "            os.mkdir(os.path.join('./resize/'+dn))\n",
    "    except OSError as e:\n",
    "        if e.errno != errno.EExist:\n",
    "            print(\"Failed to creat dir\")\n",
    "    res_img.save('./resize/'+dn+'/'+fn, \"JPEG\", quality = 95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 앞서 크롤링을 통해 다운받은 이미지 파일을 리사이징하여 새로 저장해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augimg(source, output,  sample_no):\n",
    "    p = Augmentor.Pipeline(source_directory='./'+str(source)+'/', output_directory='./'+'/output/', save_format=\"JPEG\")\n",
    "    p.flip_left_right(probability=0.2) \n",
    "    p.shear(0.3, max_shear_left = 10 , max_shear_right = 10)\n",
    "    p.random_brightness(0.4, min_factor = 0.2, max_factor = 0.6)\n",
    "    p.random_color(0.3, min_factor = 0.5, max_factor = 0.8)\n",
    "    p.random_contrast(0.3, min_factor = 0.5, max_factor = 0.8)\n",
    "    p.random_distortion(0.5, grid_width = 5, grid_height = 5, magnitude = 10)\n",
    "    p.black_and_white(0.1) \n",
    "    p.rotate(0.3, 5, 20) \n",
    "    p.skew(0.3, 0.5) \n",
    "    p.zoom(probability = 0.3, min_factor =0.2, max_factor = 0.8)\n",
    "    p.crop_random(0.3, 0.7)\n",
    "    p.sample(sample_no) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in glob.glob('./resize/*/'):\n",
    "    dn = os.path.dirname(x).split('/')[-1]\n",
    "    print(dn)\n",
    "    augimg('./resize/'+dn+'/','output',100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 리사이징한 이미지를 토대로 augmentation을 진행 각 그림별로 100개씩 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이미지 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = []\n",
    "y_data = []\n",
    "n = 0\n",
    "\n",
    "for x in glob.glob('./resize/*/output/*'):\n",
    "    dn = os.path.dirname(x).split('/')[-2]\n",
    "    dn = [key for key, value in fmp_name.items() if value == dn]\n",
    "    img = image.load_img(x, target_size = (299, 299))\n",
    "    img_tensor = image.img_to_array(img)\n",
    "    img_tensor /= 255.\n",
    "    X_data.append(img_tensor)\n",
    "    y_data.append(dn[0])\n",
    "    n += 1\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train data, test data를 만들기 위한 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(X_data), y_data, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 사이킷 런에 포함된 train_test_split 기능을 이용해 무작위로 학습 데이터를 선정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train : ', X_train.shape,'X_test : ', X_test.shape, '\\ny_train : ', len(y_train), ' y_test : ',len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cat = to_categorical(np.array(y_train), num_classes = 61)\n",
    "y_test_cat = to_categorical(np.array(y_test), num_classes = 61)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 리스트 형태인 y 데이터를 컴퓨터가 학습할 수 있도록 to_categorical을 사용하여 one-hot encoding을 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xception.Xception(include_top=True, weights=None, input_tensor=None, input_shape=(299,299,3), pooling='avg', classes=61)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- xception 모델 로드\n",
    "    - xception 모델의 경우 imagenet 경연의 우승 모델중 하나로 모델자체가 keras에 구축되어 있다.\n",
    "    - 이러한 모델을 로드해오는 방법으로 사용\n",
    "- 모델의 학습을 위해 컴파일 해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_model = model.fit(X_train, y_train_cat, epochs=200, batch_size=16, validation_data=(X_test, y_test_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 데이터를 통해 모델을 학습해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./totalsave.h5')\n",
    "model.save_weights('./sw_again.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- model.save 는 모델의 구조까지 모두 저장하는 방법으로 기존 로드한 모델에 새로운 구조를 추가시켰을때나 모델의 구조가 변경되었을때 사용할 수 있다.\n",
    "- model.save_weights 는 모델의 가중치, 즉 학습정도만을 저장하는것으로 모델이 기본 구조를 유지하고 있을때 사용할 수 있다.\n",
    "    - 전체 저장보다는 가볍고 용량도 작게 차지한다는 장점이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./model0_4.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델의 학습정도만을 불러오는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = fmp_name[17]\n",
    "\n",
    "img = image.load_img('/content/drive/My Drive/Git/second/resize/'+a+'/'+a+'.jpg', target_size=(299,299))\n",
    "\n",
    "target = []\n",
    "img_tensor = image.img_to_array(img)\n",
    "img_tensor /= 255.\n",
    "target.append(img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(model.predict(np.array(target))[0])\n",
    "\n",
    "print(max(x))\n",
    "fmp_name[x.index(max(x))].rsplit(' by')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델의 정확도를 검증하기 위한 확인 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.google.com/search?q='\n",
    "paint_name = fmp_name[x.index(max(x))].rsplit(' by')[0]\n",
    "m = parse.quote(paint_name)\n",
    "#params = {\n",
    "#    'q': 'The Embarkation of the Queen of Sheba by Claude Lorrain',\n",
    "#}\n",
    "\n",
    "response = requests.get(url+m, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "html = response.text\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "#print(soup)\n",
    "title_list = soup.find_all('div', class_ = 'BNeawe deIvCb AP7Wnd')\n",
    "#img_info = soup.find(class_ = 'image').find('img')\n",
    "#img_src = img_info.get('src')\n",
    "#title_info = soup.find('table').find_all('td')\n",
    "#print('http:'+img_src)\n",
    "#print(title_list)\n",
    "for tag in title_list:\n",
    "    print(tag.text)\n",
    "\n",
    "real_paint_name = title_list[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 처음 데이터를 가져온 사이트에서 그림의 이름을 축약하여 적어둔 경우가 존재함.\n",
    "- 이를 처리하기 위해 구글 검색 크롤링을 통해 온전한 이름 획득"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/'\n",
    "#m = parse.quote(paint_name)\n",
    "#params = {\n",
    "#    'q': 'The Embarkation of the Queen of Sheba by Claude Lorrain',\n",
    "#}\n",
    "\n",
    "response = requests.get(url+real_paint_name)\n",
    "html = response.text\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "#print(soup)\n",
    "title_list_R = soup.find('table').find_all('th')\n",
    "img_info = soup.find(class_ = 'image').find('img')\n",
    "img_src = img_info.get('src')\n",
    "title_info = soup.find('table').find_all('td')\n",
    "#print('http:'+img_src)\n",
    "#print(title_list)\n",
    "for tag,info in zip(title_list, title_info):\n",
    "    print(tag.text, '  ', info.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 구글 검색을 통해 찾아낸 온전한 이름을 통해 그림의 정보를 크롤링해옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
